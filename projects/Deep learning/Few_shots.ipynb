{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6WHbNVaa22P"
   },
   "source": [
    "# Few-shot Adaptation: A Review of Different Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqSpI87ha22R"
   },
   "source": [
    "**Damiano Orlandi**\n",
    "\n",
    "\n",
    "**Tommaso Rondani**\n",
    "\n",
    "\n",
    "**Raffaele Sinani**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oj3ASrqra22R"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this work is to explore different methods to perform few-shot adaptation of a pre-trained CLIP model. Few-shot adaptation allows a model to improve its performance on a number of base classes, of which some examples are provided, while retaining the general performance of the pre-trained model on other novel, unseen classes.\n",
    "\n",
    "The methods we proposed are a confidence-based thresholding methods, an anomaly detection-based approach and an architecture based on Topological Data Analysis.\n",
    "\n",
    "The outline of this paper is as follows. First, we briefly introduce the methodologies used, as well as specifying some design choices we employed. Then, we present our code implementation, showing and explaining the necessary steps to reproduce our results. Lastly, we present the results obtained and provide some comments, comparing the advantages and disadvantages of the techniques used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKjKnFuja22S"
   },
   "source": [
    "## Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyLph92Sa22S"
   },
   "source": [
    "### Confidence-based Thresholding\n",
    "\n",
    "The first, simplest method that we employed was confidence-based thresholding.We initially trained a linear classifier on the 51 classes from the base set, using the frozen, pretrained ResNet-50 CLIP backbone as feature extractor, while keeping the CLIP text encoder fixed.  A threshold value was selected using the validation set. At inference time, new entries were classified according to the following criterion: if the softmaxed confidence level of the classifier surpassed the threshold, then that prediction is utilized. Otherwise, we fell back on a similarity-based approach: we computed the cosine similarity between the image features and the text features, mimicking the behavior of the original CLIP model.\n",
    "\n",
    "If an appropriate number of training epochs and a well-tuned threshold are selected, the worst-case performance of the model should approximate that of the original CLIP. A sufficiently high threshold ensures that the classifier is only used for examples where it is highly confident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F43jCo__a22S"
   },
   "source": [
    "### Anomaly Detection-based methods\n",
    "\n",
    "The second approach we experimented with involved anomaly detection techniques.  \n",
    "We explored two different methodologies:\n",
    "\n",
    "- **A simple Gaussian cluster-based method:**  \n",
    "  After computing and storing the centroid of the extracted features for each class,  \n",
    "  we calculated the distance from each new input to every class centroid.  \n",
    "  A prediction was made based on whether the distance fell below a threshold,  \n",
    "  which was determined using the standard deviation of the training examples within each class.\n",
    "\n",
    "- **A hierarchical generative model:**  \n",
    "  A more complex, hierarchical generative model was applied to the input images  \n",
    "  to distinguish between base and novel classes.  \n",
    "  This approach should enable selective use of finetuned methods for base classes  \n",
    "  and CLIP for novel classes.  \n",
    "  The model is described in [Sheynin et al., (2021)](https://arxiv.org/abs/2104.14535) and was adapted for training on the Flowers102 dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JY0KWAGta22T"
   },
   "source": [
    "### TopoVPT with thresholding\n",
    "\n",
    "The last approach attempted to bridge deep learning and Topological Data Analysis (TDA). By using persistence images ([Adams et al., 2015](https://arxiv.org/abs/1507.06217)) as prompts for the VPT ([Jia et al., 2022](https://arxiv.org/abs/2203.12119)), we expected to obtain an improvement on the accuracy for the images in the base classes. Persistence images, without delving into a heavily mathematical explanation (which is found [here](https://arxiv.org/abs/2312.05840)), may be considered as topological filter maps, which focus on understanding the creation and dissolution of, in this case, 0-dimensional and 1-dimensional cavities and transform the obtained data into an image. Due to the novelty of this technique, only black-and-white images can be processed, since multiple channels would require the usage of multiparameter homology. In order to achieve that, the natural topological structure is the cubical homology, which treats each pixel as a vertex of a discrete lattice. This model was adopted by the work -- not yet published -- done by Casacuberta and Ferrà Marcus on cardiovascular magnetic resonance imaging.\n",
    "\n",
    "Two versions of this model have been proposed: one in which only the prompt injections parameters are trained and one in which also the parameters of the third and fourth layer are trained.\n",
    "\n",
    "The main limitation consists of the fact that methods arisen from TDA are computationally intensive, do not have yet a GPU-based implementation, and require a large amount images to capture the necessary information. Moreover, more complex topological tool, such as multiparameter homology, would be more appropriate to capture the RGB colouring of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRfJLN06a22T"
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYQiprE0pcVD"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzXtFjhh7iOS",
    "outputId": "a0fbcb3f-8bf4-46ab-82d8-d26133e6bf62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai_clip in /opt/conda/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.12/site-packages (from openai_clip) (6.3.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.12/site-packages (from openai_clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from openai_clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from ftfy->openai_clip) (0.2.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 14:24:45.234747: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-22 14:24:45.248785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755872685.266941    4743 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755872685.272575    4743 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-22 14:24:45.291995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai_clip   #This is required on AWS as clip is not naturally installed\n",
    "%pip install gudhi\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import skimage\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import itertools\n",
    "import gudhi\n",
    "import gudhi.representations\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2353MHw1p24h"
   },
   "source": [
    "### Dataset Loading\n",
    "The Flowers102 dataset can directly be downloaded via torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_1CrUhZpVCq"
   },
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJI_a5EizA5a"
   },
   "source": [
    "### Base and Novel categories\n",
    "To split in base and novel categories we list all dataset classes, and count their number.\n",
    "Then, we just allocate the first half to base categories and the remaining half to novel ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfq51vd8q_5a"
   },
   "outputs": [],
   "source": [
    "def base_novel_categories(dataset):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes, all_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvDdoYQr2fIu"
   },
   "source": [
    "### Inspect Classes\n",
    "We can now visualize which are the base and novel classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veGGpNDctCgR",
    "outputId": "9d8661df-c946-46e6-db96-701644602fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Class Names: [(0, 'pink primrose'), (1, 'hard-leaved pocket orchid'), (2, 'canterbury bells'), (3, 'sweet pea'), (4, 'english marigold'), (5, 'tiger lily'), (6, 'moon orchid'), (7, 'bird of paradise'), (8, 'monkshood'), (9, 'globe thistle'), (10, 'snapdragon'), (11, \"colt's foot\"), (12, 'king protea'), (13, 'spear thistle'), (14, 'yellow iris'), (15, 'globe-flower'), (16, 'purple coneflower'), (17, 'peruvian lily'), (18, 'balloon flower'), (19, 'giant white arum lily'), (20, 'fire lily'), (21, 'pincushion flower'), (22, 'fritillary'), (23, 'red ginger'), (24, 'grape hyacinth'), (25, 'corn poppy'), (26, 'prince of wales feathers'), (27, 'stemless gentian'), (28, 'artichoke'), (29, 'sweet william'), (30, 'carnation'), (31, 'garden phlox'), (32, 'love in the mist'), (33, 'mexican aster'), (34, 'alpine sea holly'), (35, 'ruby-lipped cattleya'), (36, 'cape flower'), (37, 'great masterwort'), (38, 'siam tulip'), (39, 'lenten rose'), (40, 'barbeton daisy'), (41, 'daffodil'), (42, 'sword lily'), (43, 'poinsettia'), (44, 'bolero deep blue'), (45, 'wallflower'), (46, 'marigold'), (47, 'buttercup'), (48, 'oxeye daisy'), (49, 'common dandelion'), (50, 'petunia')]\n",
      "Novel Class Names: [(51, 'wild pansy'), (52, 'primula'), (53, 'sunflower'), (54, 'pelargonium'), (55, 'bishop of llandaff'), (56, 'gaura'), (57, 'geranium'), (58, 'orange dahlia'), (59, 'pink-yellow dahlia?'), (60, 'cautleya spicata'), (61, 'japanese anemone'), (62, 'black-eyed susan'), (63, 'silverbush'), (64, 'californian poppy'), (65, 'osteospermum'), (66, 'spring crocus'), (67, 'bearded iris'), (68, 'windflower'), (69, 'tree poppy'), (70, 'gazania'), (71, 'azalea'), (72, 'water lily'), (73, 'rose'), (74, 'thorn apple'), (75, 'morning glory'), (76, 'passion flower'), (77, 'lotus'), (78, 'toad lily'), (79, 'anthurium'), (80, 'frangipani'), (81, 'clematis'), (82, 'hibiscus'), (83, 'columbine'), (84, 'desert-rose'), (85, 'tree mallow'), (86, 'magnolia'), (87, 'cyclamen'), (88, 'watercress'), (89, 'canna lily'), (90, 'hippeastrum'), (91, 'bee balm'), (92, 'ball moss'), (93, 'foxglove'), (94, 'bougainvillea'), (95, 'camellia'), (96, 'mallow'), (97, 'mexican petunia'), (98, 'bromelia'), (99, 'blanket flower'), (100, 'trumpet creeper'), (101, 'blackberry lily')]\n"
     ]
    }
   ],
   "source": [
    "_, _, tmp_test = get_data()\n",
    "base_classes, novel_classes, all_classes = base_novel_categories(tmp_test)\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
    "print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8puO1VNpzwvi"
   },
   "source": [
    "### Split Dataset\n",
    "The next step is to actually split the dataset into the base and novel categories we extract from `base_novel_categories`.\n",
    "To split the data we need the dataset and the list of base classes. If the sample label is not part of the base categories, then it must be part of the novel ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msOszMs2zRRu"
   },
   "outputs": [],
   "source": [
    "def split_data(dataset, base_classes):\n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    # we create a set of base classes to compute the test below in O(1)\n",
    "    # this is optional and can be removed\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    # here we create the dataset subsets\n",
    "    # the torch Subset is just a wrapper around the dataset\n",
    "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
    "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
    "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQZT22rE8hBw"
   },
   "source": [
    "### Extract k shots\n",
    "As the dataset already provides 10 train and validation shots, we do not need to extract them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KpbPRLr7WL_"
   },
   "source": [
    "### Load CLIP\n",
    "\n",
    "Using the `clip` module we can import the default image preprocessing for CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sh6uLZRT7YJx"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "_, preprocess = clip.load(\"RN50\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM9H14899ses"
   },
   "source": [
    "### Load and Prepare Data\n",
    "Here we get the three dataset split and pass CLIP pre-defined augmentations. Then, we compute base and novel categories. Finally, se split the three datasets into base and novel categories.\n",
    "The novel categories are excluded from the training set but are conserved for testing and validation sets. We opted to maintain novel examples for validation because we did not want to test hyperparameters on the test set, as it would artificially inflate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVrYUYTv9ttM"
   },
   "outputs": [],
   "source": [
    "# get the three datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# split classes into base and novel\n",
    "base_classes, novel_classes, all_classes = base_novel_categories(train_set)\n",
    "\n",
    "# split the three datasets\n",
    "train_base, _ = split_data(train_set, base_classes)\n",
    "val_base, val_novel = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCqhcFRoa22W"
   },
   "source": [
    "### Creating a function to convert to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mhsvnsWa22W"
   },
   "outputs": [],
   "source": [
    "def rgb_to_grayscale(image: torch.Tensor) -> torch.Tensor:\n",
    "    if image.ndim != 3 or image.shape[0] != 3:\n",
    "        raise ValueError(\"Expected image tensor of shape (3, H, W)\")\n",
    "    r, g, b = image[0:1], image[1:2], image[2:3]\n",
    "    return 0.299 * r + 0.587 * g + 0.114 * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1oGk6Sma22W"
   },
   "source": [
    "### Defining a function to create the persistence images\n",
    "\n",
    "The function utilizes the `gudhi` library to generate the persistence images from the input images. They are then injected as prompts in the TopoVPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IeJweW1a22W"
   },
   "outputs": [],
   "source": [
    "def topo_hammer(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Return a **50×50** persistence image (H₁) as float32 CPU tensor.\"\"\"\n",
    "    # Tranform the input image to grayscale\n",
    "    gray = rgb_to_grayscale(img).squeeze(0).cpu().numpy()\n",
    "    # Create cubical complex\n",
    "    cubical = gudhi.CubicalComplex(top_dimensional_cells=gray)\n",
    "    # Compute persistence diagram\n",
    "    d = cubical.persistence()\n",
    "    # Keep only the necessary data\n",
    "    d = np.array([interval for (dim,interval) in d])\n",
    "    # Remove np.inf\n",
    "    d = d[~np.isinf(d).any(axis=1)]\n",
    "    # Instantiate persistence image\n",
    "    pim = gudhi.representations.PersistenceImage(\n",
    "        resolution=[50, 50], bandwidth=0.1, weight=lambda pt: pt[1] ** 2\n",
    "    )\n",
    "    # Compute persistence image\n",
    "    vec = pim.fit_transform([d])[0].astype(np.float32)\n",
    "    # Return a torch.tensor\n",
    "    return torch.tensor(vec).view(50, 50)# 50 prompts × 50 dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3V4iElza22W"
   },
   "source": [
    "### Creating our CLIP models\n",
    "\n",
    "We create now the clip model used for the clustering and thresholding methodologies. The two different modes can be exchanged on model initialization using the `crit` parameter.\n",
    "We start from the baseline ResNet50 clip backbone and add a linear classifier to implement thresholding. To implement clustering we also need to initialize some more parameters that are collected during training to conserve the cluster centroids and distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTt5RFfQa22W"
   },
   "source": [
    "The following code can be used to initialize both Threshold and Clustering models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "Yw8ckEfBR8Tn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ThresholdCLIP(nn.Module):\n",
    "    def __init__(self, text_features = 0, exnumber: int = 10, num_classes: int = 102, crit = \"thresholding\"):\n",
    "        super().__init__()\n",
    "        #Initialize the CLIP visual encoder\n",
    "        self.model, _ = clip.load(\"RN50\")\n",
    "        self.model = self.model.float()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        #The original clip text and visualk encoder is maintained\n",
    "        self.text_encoder = self.model.encode_text\n",
    "        self.encoder = self.model.visual\n",
    "\n",
    "        #The linear classifier is implemented. We utilize a size of 102, even though the base classes are 51 so that the tensor shape is still 102, matching the testing scenario\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "\n",
    "        #This conserves the type of model we are going to utilize\n",
    "        self.criterion = crit\n",
    "\n",
    "        #This initializes a default threshold, used for both clustering and thresholding\n",
    "        self.threshold = 0.3\n",
    "\n",
    "        #Initialize means and distribution tensors.\n",
    "        if self.criterion == \"clustering\":\n",
    "            self.distributions = torch.Tensor(num_classes, exnumber, 1024).to(device)\n",
    "            self.means = torch.Tensor(num_classes, 1024).to(device)\n",
    "            self.stddevs = torch.Tensor(num_classes).to(device)\n",
    "            self.average_distances = torch.zeros(num_classes).to(device)\n",
    "\n",
    "    def freeze_encoders(self):\n",
    "        '''\n",
    "        This function freezes both text and image encoders so that they are not updated during training\n",
    "        '''\n",
    "        #freeze the image encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        #freeze text encoder as well\n",
    "        for param in self.model.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor, text_features=None) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        if self.training:\n",
    "            # Save encoded feature for access outside\n",
    "            self.x_distr = x.detach().clone()  # for storing later\n",
    "            x = self.classifier(x)\n",
    "\n",
    "            return x\n",
    "        else:\n",
    "            if self.criterion == \"thresholding\":\n",
    "                tmp = self.classifier(x)\n",
    "                results = torch.zeros(x.shape[0], self.num_classes, device=x.device)\n",
    "                for i in range(x.shape[0]):\n",
    "                    if F.softmax(tmp[i], dim=0).max() > self.threshold:  #This compares the confidence of the prediction with the threshold\n",
    "                        results[i] = tmp[i]\n",
    "\n",
    "                    else: #Regular CLIP forward pass\n",
    "                        x[i] /= x[i].norm(dim=-1, keepdim=True)\n",
    "\n",
    "                        # Compute cosine similarity with text prompts\n",
    "                        sims = x[i] @ text_features.T\n",
    "                        results[i] = sims\n",
    "\n",
    "            elif self.criterion == \"clustering\":\n",
    "                tmp = self.classifier(x)\n",
    "                results = torch.zeros(x.shape[0], self.num_classes, device=x.device)\n",
    "                for i in range(x.shape[0]):\n",
    "                    if self.detect_class_distance(x[i]): #We see if it belongs to any cluster\n",
    "                        results[i] = tmp[i]\n",
    "\n",
    "                    else:  #Regular CLIP forward pass\n",
    "                        x[i] /= x[i].norm(dim=-1, keepdim=True)\n",
    "\n",
    "                        # Compute cosine similarity with text prompts\n",
    "                        sims = x[i] @ text_features.T  # [batch_size, num_classes]\n",
    "                        results[i] = sims\n",
    "            return results\n",
    "\n",
    "    def detect_class_distance(self, x: torch.Tensor):\n",
    "        '''\n",
    "        Computes the distances between the features tensor and each cluster centroid and compares it to the threshold\n",
    "        '''\n",
    "        for i in range(self.num_classes//2):\n",
    "            dist = torch.nn.functional.pairwise_distance(x.unsqueeze(0), self.means[i,:].unsqueeze(0)).item()\n",
    "            if dist < self.stddevs[i]*self.threshold:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def encode_text(self, text: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Encodes the text and normalizes\n",
    "        '''\n",
    "        text_features = self.text_encoder(text)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIMiX3QLa22X"
   },
   "source": [
    "The following code can be used to initialize our TopoVPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRqxcW6da22X"
   },
   "outputs": [],
   "source": [
    "class ResNet50Prompted(nn.Module):\n",
    "    PROMPTS = 50  # number of prompts (rows)\n",
    "    PDIM    = 50  # dimension of each prompt (columns)\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model, _ = clip.load(\"RN50\")\n",
    "        model = model.float()\n",
    "        self.base = model.visual\n",
    "        self.conv1 = self.base.conv1\n",
    "        self.bn1 = self.base.bn1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv2 = self.base.conv2\n",
    "        self.bn2 = self.base.bn2\n",
    "        self.conv3 = self.base.conv3\n",
    "        self.bn3 = self.base.bn3\n",
    "        ##\n",
    "        self.layer1 = self.base.layer1\n",
    "        self.layer2 = self.base.layer2\n",
    "        self.layer3 = self.base.layer3\n",
    "        self.layer4 = self.base.layer4\n",
    "\n",
    "        self.avgpool = self.base.avgpool\n",
    "\n",
    "        self.attnpool = self.base.attnpool\n",
    "\n",
    "        # lift prompt‑dim (50) → feature channels\n",
    "        self.lift2 = nn.Conv2d(self.PDIM, 512, 1, bias=False)\n",
    "        self.lift3 = nn.Conv2d(self.PDIM, 1024, 1, bias=False)\n",
    "        # adapters to restore original channel size after concat\n",
    "        self.adapter2 = nn.Conv2d(512 + 512, 512, 1, bias=False)\n",
    "        self.adapter3 = nn.Conv2d(1024 + 1024, 1024, 1, bias=False)\n",
    "\n",
    "        # Initialize parameters for lift2 and adapter2\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights for lift2\n",
    "        init.kaiming_normal_(self.lift2.weight, mode='fan_out', nonlinearity='relu', )  # He initialization\n",
    "        self.lift2.weight.data *= 0.00001\n",
    "        if self.lift2.bias is not None:\n",
    "            init.constant_(self.lift2.bias, 0)  # Initialize bias to zero if it exists\n",
    "\n",
    "        # Initialize weights for adapter2\n",
    "        conv_layer = self.adapter2  # Get the Conv2d layer from the Sequential\n",
    "        init.kaiming_normal_(conv_layer.weight, mode='fan_out', nonlinearity='relu')  # He initialization\n",
    "        conv_layer.weight.data *= 0\n",
    "        if conv_layer.bias is not None:\n",
    "            init.constant_(conv_layer.bias, 0)  # Initialize bias to zero if it exists\n",
    "\n",
    "        #Initialize weights for lift2\n",
    "        init.kaiming_normal_(self.lift3.weight, mode='fan_out', nonlinearity='relu', )  # He initialization\n",
    "        self.lift3.weight.data *= 0\n",
    "        if self.lift3.bias is not None:\n",
    "            init.constant_(self.lift2.bias, 0)  # Initialize bias to zero if it exists\n",
    "\n",
    "        # Initialize weights for adapter2\n",
    "        conv_layer = self.adapter3  # Get the Conv2d layer from the Sequential\n",
    "        init.kaiming_normal_(conv_layer.weight, mode='fan_out', nonlinearity='relu')  # He initialization\n",
    "        conv_layer.weight.data *= 0.00001\n",
    "        if conv_layer.bias is not None:\n",
    "            init.constant_(conv_layer.bias, 0)\n",
    "\n",
    "    def _prompt_map(self, img: torch.Tensor, H: int, W: int, lift: nn.Conv2d, C_out: int):\n",
    "        \"\"\"Compute lifted prompt map (1×C_out×H×W) from a single image.\"\"\"\n",
    "        prompts = topo_hammer(img).to(img.device)  # 50×50\n",
    "\n",
    "        P, Dp = prompts.shape  # 50,50\n",
    "        # Broadcast each prompt over spatial dims and sum across P\n",
    "        p = prompts.view(P, Dp, 1, 1).expand(-1, -1, H, W).sum(dim=0, keepdim=True)  # 1×Dp×H×W\n",
    "\n",
    "        mean = p.mean(dim=(2, 3), keepdim=True)\n",
    "        std = p.std(dim=(2, 3), keepdim=True) + 1e-6  # add epsilon to prevent divide-by-zero\n",
    "        p = (p - mean) / std\n",
    "\n",
    "        return lift(p)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B = x.size(0)\n",
    "\n",
    "        # First convolution\n",
    "        conv1_out = self.conv1(x)\n",
    "\n",
    "        # Batch normalization\n",
    "        bn1_out = self.bn1(conv1_out)\n",
    "\n",
    "        # ReLU\n",
    "        relu_out = self.relu(bn1_out)\n",
    "\n",
    "        # Second convolution\n",
    "        conv2_out = self.conv2(relu_out)\n",
    "\n",
    "        # Batch normalization\n",
    "        bn2_out = self.bn2(conv2_out)\n",
    "\n",
    "        # ReLU\n",
    "        relu_out = self.relu(bn2_out)\n",
    "\n",
    "        # Third convolution\n",
    "        conv3_out = self.conv3(relu_out)  # use relu_out from above\n",
    "\n",
    "        # Batch normalization\n",
    "        bn3_out = self.bn3(conv3_out)\n",
    "\n",
    "        # ReLU\n",
    "        relu_out = self.relu(bn3_out)\n",
    "\n",
    "        #Average pooling\n",
    "        h = self.avgpool(relu_out)\n",
    "\n",
    "        # Layer1\n",
    "        h = self.layer1(h)\n",
    "\n",
    "        # Layer2 with prompts\n",
    "        h = self.layer2(h)\n",
    "\n",
    "        _, _, H2, W2 = h.shape\n",
    "\n",
    "        # Prompt mapping\n",
    "        p2 = torch.cat([self._prompt_map(x[i], H2, W2, self.lift2, 512) for i in range(B)], dim=0)\n",
    "        p2 = F.normalize(p2, dim=1)\n",
    "\n",
    "        # Concatenate and adapt\n",
    "        h = self.adapter2(torch.cat([p2, h], dim=1)) + h #We add residual connections\n",
    "\n",
    "        # Layer3 with prompts\n",
    "        h = self.layer3(h)\n",
    "\n",
    "        _, _, H3, W3 = h.shape\n",
    "        p3 = torch.cat([self._prompt_map(x[i], H3, W3, self.lift3, 1024) for i in range(B)], dim=0)\n",
    "        p3 = F.normalize(p3, dim=1)\n",
    "\n",
    "        h = self.adapter3(torch.cat([p3, h], dim=1)) + h  #We add residual connections\n",
    "\n",
    "        # Layer 4\n",
    "        h = self.layer4(h)\n",
    "\n",
    "        #Attention pooling\n",
    "        h = self.attnpool(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "class CLIPWithTopoPrompts(nn.Module):\n",
    "    def __init__(self, num_classes: int = 102, retrain = False):\n",
    "        super().__init__()\n",
    "        self.orig, _ = clip.load(\"RN50\")\n",
    "        self.orig = self.orig.float()\n",
    "        self.num_classes = num_classes\n",
    "        self.visual = ResNet50Prompted()\n",
    "        self.retrain = retrain\n",
    "\n",
    "        self.text_encoder = self.orig.encode_text\n",
    "        self.threshold = 0.014\n",
    "\n",
    "    def freeze_encoders(self):\n",
    "        for p in self.visual.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in itertools.chain(self.visual.lift2.parameters(),\n",
    "                         self.visual.lift3.parameters(),\n",
    "                         self.visual.adapter2.parameters(),\n",
    "                         self.visual.adapter3.parameters(),\n",
    "                                ):\n",
    "            p.requires_grad = True\n",
    "        #Utilize the retrain option when initializing to train the parameters of layer 3 and 4\n",
    "        if self.retrain:\n",
    "            for p in itertools.chain(\n",
    "                self.visual.layer3.parameters(),\n",
    "                self.visual.layer4.parameters(),\n",
    "            ):\n",
    "                p.requires_grad = True\n",
    "        #freeze text encoder as well\n",
    "        for param in self.orig.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def encode_text(self, text: torch.Tensor) -> torch.Tensor:\n",
    "        text_f = self.text_encoder(text)\n",
    "        norm = text_f.norm(dim=-1, keepdim=True)\n",
    "        epsilon = 1e-8  # Small value to prevent division by zero\n",
    "        text_features = text_f / (norm + epsilon)\n",
    "        return text_features\n",
    "\n",
    "    def forward(self, x: torch.Tensor, text_features=None) -> torch.Tensor:\n",
    "        img = self.visual(x)\n",
    "        img_norm = img.norm(dim=-1, keepdim=True)\n",
    "        image_features_normed = img / img_norm\n",
    "\n",
    "        if self.training:\n",
    "            sims = image_features_normed @ text_features.T\n",
    "            return sims\n",
    "\n",
    "        else:\n",
    "            tmp = (image_features_normed @ text_features.T)\n",
    "            results = torch.zeros(x.shape[0], self.num_classes, device=x.device)\n",
    "            for i in range(x.shape[0]):\n",
    "                if F.softmax(tmp[i], dim=0).max() > self.threshold:\n",
    "                    results[i] = tmp[i]\n",
    "\n",
    "                else:\n",
    "                    # forward image through CLIP image encoder\n",
    "                    image = x[i].unsqueeze(0)\n",
    "                    img[i] = self.orig.encode_image(image)\n",
    "                    # and normalize\n",
    "                    img[i] /= img[i].norm(dim=-1, keepdim=True)\n",
    "\n",
    "                    # Compute cosine similarity with text prompts\n",
    "                    sims = img[i] @ text_features.T\n",
    "                    results[i] = sims\n",
    "            return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpHGc333a22X"
   },
   "source": [
    "### Freeze Batch Normalization stats\n",
    "\n",
    "We define a function to freeze batchnorm stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZCrD7asa22X"
   },
   "outputs": [],
   "source": [
    "def freeze_batchnorm_stats(model: nn.Module):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):\n",
    "            module.eval()\n",
    "            module.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTtMh_l4a22X"
   },
   "source": [
    "### Define Training and Evaluation routines\n",
    "\n",
    "We now define our training and evaluation routines. The evaluation routines are going to run in `eval` mode. The text features can be encoded once at the beginning of the routines as they are shared for all images. The loss we selected is Cross Entropy, the default loss for classification tasks. When training the topologic vpt the freeze parameter should be set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ysw9ZH4Va22X"
   },
   "outputs": [],
   "source": [
    "def training_step_thresh(net, data_loader, optimizer, cost_function, device=\"cuda\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "    class_counts = torch.zeros(net.num_classes, dtype=torch.long).to(device)\n",
    "\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "    #Freeze both encoders and batchnorm stats\n",
    "    net.freeze_encoders()\n",
    "    freeze_batchnorm_stats(net)\n",
    "\n",
    "\n",
    "    # Iterate over the training set\n",
    "    pbar = tqdm(data_loader, desc=\"Training\", position=0, leave=True, total=len(data_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.float().to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        if net.criterion == \"clustering\":\n",
    "            for i in range(inputs.size(0)):\n",
    "                label = targets[i]\n",
    "                count = class_counts[label]\n",
    "\n",
    "                if count < net.distributions.shape[1]:\n",
    "                    net.distributions[label, count] = net.x_distr[i]\n",
    "                    class_counts[label] += 1\n",
    "\n",
    "        # Loss computation\n",
    "\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Fetch prediction and loss value\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_postfix(train_loss=loss.item(), train_acc=cumulative_accuracy / samples * 100)\n",
    "        pbar.update(1)\n",
    "\n",
    "    if net.criterion == \"clustering\":\n",
    "        # Compute the per-class means and stddevs of distances\n",
    "        for i in range(net.num_classes):\n",
    "            if class_counts[i] > 0:\n",
    "                # Compute class mean\n",
    "                net.means[i] = net.distributions[i].mean(dim=0)\n",
    "\n",
    "                # Compute standard deviation of distances from class mean\n",
    "                diffs = net.distributions[i] - net.means[i]\n",
    "                dists = torch.norm(diffs, dim=1)\n",
    "                net.stddevs[i] = dists.std()\n",
    "                net.average_distances[i] = dists.mean()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
    "\n",
    "\n",
    "def training_step_VPT(net, data_loader, optimizer, cost_function, categories, device=\"cuda\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "    class_counts = torch.zeros(net.num_classes, dtype=torch.long).to(device)\n",
    "\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "    #Freeze both encoders\n",
    "    net.freeze_encoders()\n",
    "    #Batch norm stats shoudldn't be frozen when training the vpt\n",
    "\n",
    "\n",
    "    #Encoding the text prompts is required for the TopoVPT training\n",
    "    with torch.no_grad():\n",
    "        text_inputs = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
    "        text_features = net.encode_text(text_inputs).to(device, dtype=torch.float32)\n",
    "        norm = text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / norm\n",
    "\n",
    "    # Iterate over the training set\n",
    "    pbar = tqdm(data_loader, desc=\"Training\", position=0, leave=True, total=len(data_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.float().to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs, text_features)\n",
    "\n",
    "\n",
    "        # Loss computation\n",
    "\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Fetch prediction and loss value\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_postfix(train_loss=loss.item(), train_acc=cumulative_accuracy / samples * 100)\n",
    "        pbar.update(1)\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
    "\n",
    "\n",
    "def eval_step(net, data_loader, cost_function, num_classes, categories, device=\"cuda\"):\n",
    "    net.eval()\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_inputs = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
    "        text_features = net.encode_text(text_inputs).to(device, dtype=torch.float32)\n",
    "        norm = text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / norm\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    norm_weights = torch.zeros(num_classes, 1024).to(device)\n",
    "    maxacc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test set\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            # Load data into GPU\n",
    "            contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "            targets = torch.Tensor([contig_cat2idx[t.item()] for t in targets]).long()\n",
    "\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Forward pass\n",
    "            outputs = net(inputs, text_features)\n",
    "\n",
    "            # Loss computation\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += inputs.shape[0]\n",
    "            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        del text_inputs, inputs, targets #To avoid GPU memory overusage\n",
    "\n",
    "        return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ2uyMT0a22Y"
   },
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww042l9Ya22Y"
   },
   "source": [
    "To initialize the threshold model use the option crit = \"thresholding\", and crit = \"clustering\" for clustering model.\n",
    "\n",
    "\n",
    "In case of our TopoVPT retrain can be specified as true or false. If true the 3rd and 4th layer parameters are also going to be retrained. If false only the prompt injection ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCxWnHsfa22Y"
   },
   "outputs": [],
   "source": [
    "net = ThresholdCLIP(exnumber=10, num_classes=102, crit = \"thresholding\").to(device)\n",
    "#net = CLIPWithTopoPrompts(retrain=True).to(device)\n",
    "#net = CLIPWithTopoPrompts(retrain=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDiGCwwda22a"
   },
   "source": [
    "### Hyperparameter selection\n",
    "\n",
    "We now define some hyperparameters. For the confidence-based thresholding CLIP, the learning rate is set at 0.03, as it was justified before, it allowed fast convergence. Weight decay was kept to a default 0.0001 to avoid overfitting. For the TopoVPT model, the learning rate has been set to 0.005 and the weight decay to 0.01. Batch size was kept at 128 as the GPU memory supported it. Finally we set the maximum number of epochs to 30 as it allowed us to compare different values using checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbWNF93ba22a"
   },
   "outputs": [],
   "source": [
    "learning_rate_thresh = 0.03\n",
    "learning_rate_vpt = 0.005\n",
    "weight_decay_thresh = 1e-4\n",
    "weight_decay_vpt = 1e-2\n",
    "batch_size = 128\n",
    "epochs = 1 # Change it for a higher value (30 for thresholding, 20 for TopoVPT) to replicate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK75s4oFa22b"
   },
   "source": [
    "### Optimizer selection\n",
    "\n",
    "The optimizer we selected for the linear classifier (used in for thresholding) was the Adam optimizer. More advanced optimizers shouldn't be required as the trained network is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQ0bFrMma22b"
   },
   "outputs": [],
   "source": [
    "optimizer_thresh = torch.optim.Adam(net.parameters(), lr=learning_rate_thresh,  weight_decay=weight_decay_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L28WFYJda22b"
   },
   "source": [
    "For the TopoVPT training we utilized the AdamW optimier with a very small learning rate to avoid gradient explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKU0iIVza22b"
   },
   "outputs": [],
   "source": [
    "# Uncomment CLIPWithTopoPrompts before\n",
    "if net.retrain:\n",
    "    optimizer_VPT = opt = torch.optim.AdamW([\n",
    "        *net.visual.lift2.parameters(),\n",
    "        *net.visual.lift3.parameters(),\n",
    "        *net.visual.adapter2.parameters(),\n",
    "        *net.visual.adapter3.parameters(),\n",
    "        *net.visual.layer3.parameters(),\n",
    "        *net.visual.layer4.parameters(),\n",
    "        ], lr=learning_rate_vpt, weight_decay=weight_decay_vpt)\n",
    "else:\n",
    "    optimizer_VPT = torch.optim.AdamW([\n",
    "        *net.visual.lift2.parameters(),\n",
    "        *net.visual.lift3.parameters(),\n",
    "        *net.visual.adapter2.parameters(),\n",
    "        *net.visual.adapter3.parameters(),\n",
    "        ], lr=learning_rate_vpt, weight_decay=weight_decay_vpt)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x53lOuna22b"
   },
   "source": [
    "### Dataloader creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcAAmrMfa22b"
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train_base, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valnovelloader = torch.utils.data.DataLoader(val_novel, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "valdataloader = torch.utils.data.DataLoader(val_base, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_base_loader = torch.utils.data.DataLoader(test_base, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_novel_loader = torch.utils.data.DataLoader(test_novel, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdnfrxTQa22b"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:07:12.862029Z",
     "iopub.status.busy": "2025-08-24T13:07:12.861718Z",
     "iopub.status.idle": "2025-08-24T13:07:12.866167Z",
     "shell.execute_reply": "2025-08-24T13:07:12.865432Z",
     "shell.execute_reply.started": "2025-08-24T13:07:12.862004Z"
    },
    "id": "3R6V2SCBa22b"
   },
   "source": [
    "Thresholding/Clustering training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpieEmRda22b",
    "outputId": "020f3ce3-ecfe-491f-9850-e37f6e6a155d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s, train_acc=30, train_loss=3.43]  \n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    training_step_thresh(net=net, data_loader=dataloader, optimizer=optimizer_thresh, cost_function=torch.nn.CrossEntropyLoss(), device=device)\n",
    "    if (e+1)%5 == 0:\n",
    "        torch.save(net.state_dict(), f'model_{e+1}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0aLqig8a22b"
   },
   "source": [
    "TopoVPT training.\n",
    "\n",
    "Note: If the training is performed on Colab, it might take a very large amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wTlyokRa22b"
   },
   "outputs": [],
   "source": [
    "# Uncomment CLIPWithTopoPrompts before\n",
    "for e in range(epochs):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    training_step_VPT(net=net, data_loader=dataloader, optimizer=optimizer_VPT, cost_function=torch.nn.CrossEntropyLoss(), categories=all_classes, device=device)\n",
    "    scheduler.step()\n",
    "    if (e+1)%5 == 0:\n",
    "        torch.save(net.state_dict(), f'model_{e+1}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCPx_kp8a22b"
   },
   "source": [
    "### Checkpoint loading (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mdgmmQza22c",
    "outputId": "5c74d551-6967-4e2b-f3d7-4f671b296d58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4743/4114992321.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load('model_20.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " net.load_state_dict(torch.load('model_20.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37HJTeW7a22c"
   },
   "source": [
    "### Threshold and Model selection\n",
    "\n",
    "The threshold hyperparameter and the number of training epochs is chosen by evaluating on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJqwWvvqa22c"
   },
   "outputs": [],
   "source": [
    "net.threshold = 0.65\n",
    "# For CLIPWithTopoPrompts\n",
    "#net.threshold = 0.013 or 0.0165, depending on the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxY0cBC0a22c",
    "outputId": "0a10ae32-b690-431d-ae54-1fc28d58cf12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1480332804661171, 65.29411764705883)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_step(net, data_loader=valnovelloader,cost_function=torch.nn.CrossEntropyLoss(), device=device, num_classes = 102, categories=all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eFc1ZPQa22c",
    "outputId": "9d47feca-7a2f-42fc-9800-df5c4cc1cce4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.013900058409746955, 82.54901960784314)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_step(net, data_loader=valdataloader,cost_function=torch.nn.CrossEntropyLoss(), device=device, num_classes = 102, categories=all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDdsLfhNa22c"
   },
   "outputs": [],
   "source": [
    "_, base_accuracy = eval_step(net, data_loader=test_base_loader,cost_function=torch.nn.CrossEntropyLoss(), device=device, num_classes = 102, categories=all_classes)\n",
    "_, novel_accuracy = eval_step(net, data_loader=test_novel_loader,cost_function=torch.nn.CrossEntropyLoss(), device=device, num_classes = 102, categories=all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2Zqh3ASa22d",
    "outputId": "918e5b98-e856-4394-bfab-219418be1a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.7925596441569 67.21980413492928\n"
     ]
    }
   ],
   "source": [
    "print(base_accuracy, novel_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfgRkdGLa22d"
   },
   "source": [
    "### Define harmonic mean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7W3sYxta22d",
    "outputId": "4426f881-4201-4fcf-c627-92d7c5f4b2bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Harmonic Mean: 73.38387005217447%\n"
     ]
    }
   ],
   "source": [
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    hm = numerator / denominator\n",
    "    return hm\n",
    "\n",
    "print(f\"🔍 Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7o0-JK3a22d"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I79ruxfWa22d"
   },
   "source": [
    "### Confidence-based Thresholding\n",
    "\n",
    "Confidence-based thresholding was the simplest method we employed, but it was able to yield some gains, achieving slightly higher accuracy than the base CLIP model.Our model was trained using the Adam optimizer with learning rate of 0.03 and weight decay of 0.0001. This configuration provided a both speed and stability, allowing smooth convergence to the minimum. In contrast, lower learning rates (0.001 and 0.003) led to extremely slow convergence. Since only a simple linear classifier is actually trained, the optimization process should converge relatively smoothly.\n",
    "\n",
    "#### Validation set performance across iterations and thresholds\n",
    "\n",
    "| **# Iterations** | **Threshold** | **Base (%)** | **Novel (%)** |\n",
    "|------------------|---------------|--------------|---------------|\n",
    "|**BASE CLIP**| - | 63.92 | 68.24 |\n",
    "| 5  | 0.5  | 80.98 | 65.68 |\n",
    "| 5  | 0.65 | 72.35 | 68.04 |\n",
    "| 10 | 0.5  | 91.57 | 55.88 |\n",
    "| 10 | 0.55 | 90.00 | 58.03 |\n",
    "| 10 | 0.6  | 87.06 | 60.78 |\n",
    "| 10 | 0.65 | 85.88 | 64.12 |\n",
    "| 10 | 0.7  | 83.14 | 65.49 |\n",
    "| 15 | 0.55 | 88.63 | 59.61 |\n",
    "| 15 | 0.6  | 86.66 | 63.53 |\n",
    "| 15 | 0.65 | 84.71 | 63.92 |\n",
    "| 20 | 0.65 | 82.55 | 65.29 |\n",
    "| 20 | 0.7  | 79.02 | 66.47 |\n",
    "| 30 | 0.6  | 84.90 | 64.31 |\n",
    "| 30 | 0.65 | 82.16 | 66.27 |\n",
    "\n",
    "The table summarizes the results on the validation set. A trend can be observed, as additional training generally improves the performance on base classes, though often at the expense of novel-class accuracy. As expected, prediction confidence also increases with more epochs, requiring higher thresholds. Both training error and validation accuracy stabilize after roughly 10 epochs.\n",
    "\\\\\\\\\n",
    "The best balance was found after 15 training iterations with a threshold of 0.6. This model maintained strong performance on novel classes without a significant drop on base classes. The above mentioned trade-off can be observed at 10 iterations with a 0.65 threshold: improved novel-class accuracy at the cost of base-class performance. The same held for the 20-iteration, 0.65-threshold model, which slightly improved novel-class accuracy but again reduced base-class accuracy.\n",
    "\\\\\\\\\n",
    "By comparison at 5 iterations the model underfits, producing weaker confidence scores for base-class examples and requiring lower thresholds to show improvements over base CLIP. At 30 iterations, performance was similar but slightly worse on both base and novel classes, showing some signs of overfitting. Despite that, the model still maintains a decent performance, suggesting that further training would just plateau, rather than reducing performance.\n",
    "\n",
    "Based on the validation results, we selected models trained for 30, 35, and 40 iterations for test evaluation.\n",
    "\n",
    "#### Test set performance for selected models\n",
    "\n",
    "#### BASE CLIP\n",
    "| **Metric**       | **Value (%)** |\n",
    "|------------------|---------------|\n",
    "| Base Accuracy    | 69.47         |\n",
    "| Novel Accuracy   | 73.78         |\n",
    "| Harmonic Mean    | 71.56         |\n",
    "\n",
    "#### 10 Iterations, 0.65 Threshold\n",
    "| **Metric**       | **Value (%)** |\n",
    "|------------------|---------------|\n",
    "| Base Accuracy    | 82.37         |\n",
    "| Novel Accuracy   | 65.70         |\n",
    "| Harmonic Mean    | 73.09         |\n",
    "\n",
    "#### 15 Iterations, 0.6 Threshold\n",
    "| **Metric**       | **Value (%)** |\n",
    "|------------------|---------------|\n",
    "| Base Accuracy    | 84.63         |\n",
    "| Novel Accuracy   | 64.68         |\n",
    "| Harmonic Mean    | 73.33         |\n",
    "\n",
    "#### 20 Iterations, 0.65 Threshold\n",
    "| **Metric**       | **Value (%)** |\n",
    "|------------------|---------------|\n",
    "| Base Accuracy    | 80.79         |\n",
    "| Novel Accuracy   | 67.22         |\n",
    "| Harmonic Mean    | 73.38         |\n",
    "\n",
    "\n",
    "The thresholds selected for the three models were  0.65, 0.6 and 0.65 respectively. The 20-iteration model marginally outperformed the 15-iteration one (harmonic means of top-1 accuracy of 73.38\\% vs. 73.33\\%), though both were nearly identical and clearly improved over the base CLIP score of 71.56\\%. The 10-iteration model performed slightly worse (73.03\\%), reinforcing that additional training was beneficial. Overall, these results suggest that performance stabilized around 15 iterations, showing little improvement beyond that point. Importantly, the proposed method consistently improves upon CLIP while requiring only modest training time and memory.\n",
    "\n",
    "The last insight gained is that this methodology allows for a hyperparameter that governs the emphasis placed on base and novel classes. In our experiments, we chose a more balanced approach, aiming to maximize the harmonic mean of the accuracies. However, in a different context where maintaining novel-class accuracy was more important, the threshold could be easily adjusted accordingly. The same holds true if the goal were to maximize base-class accuracy while still maintaining reasonable performance on novel classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IvIE9S7a22d"
   },
   "source": [
    "### Anomaly Detection\n",
    "\n",
    "The anomaly detection methodologies we employed did not yield any significant result.\n",
    "\n",
    "The simple Gaussian-clustering approach was not able to distinguish any significant cluster, as the number of training examples was too low and distance to the centroid was not a good measure to utilize, as features might be heavily reweighted by the successive forward pass. Changing the value multiplying the standard deviation values causes massive shifts in the model performance. For example, using a multiplier of 0.6 in a 30-iteration model resulted in none of the base class examples being recognized, causing the model to behave like standard CLIP. In contrast, increasing the multiplier to 0.7 led to all examples being recognized, but at the cost of 0\\% accuracy on novel classes.\n",
    "\n",
    "Overall, this approach failed to match the performance of the thresholding method and significantly degraded the base CLIP model’s accuracy. It also required extensive hyperparameter tuning just to achieve reasonable results on the validation set. Moreover, the method’s reliance on storing all distances and class means in memory—necessary for computing standard deviations—resulted in substantially higher memory consumption.\n",
    "\n",
    "On the other hand, the approach based on a hierarchical transformation-discriminating generative model proved daunting under a different point of view. The model was too computationally intensive to train on both our machines and the cloud instances provided by the University. We were only able to train the model for a few iterations at a time and the performance was very lackluster when tested on models that were not trained enough. Additionally, we would be required to train a model for each of the 52 base classes, making it a lengthy and disk-consuming effort. This led us to abandon this methodology without being able to properly apply to the dataset in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWt4BgXza22d"
   },
   "source": [
    "### TopoVPT with thresholding\n",
    "\n",
    "TopoVPT proved to be a demanding model to train, due to the fact that the topological part cannot take advantage of GPU computing. It was decided to inject $50 \\times 50$ persistence images in the second and third layer of the Modified ResNet50 used by CLIP, so that the network could extract both larger and smaller-scale features from them. As has been mentioned before, since the persistence images are largely computationally intensive, it was necessary to keep them relatively small.\n",
    "\n",
    "We initialize the parameters of our model with very small values drawn from a Kaiming distribution, also known as He initialization. This approach is particularly effective for layers with ReLU activation functions, as it helps to maintain a balanced variance across layers during the forward and backward passes. By using a Kaiming distribution, we ensure that the initial weights are set in a way that mitigates issues like vanishing or exploding gradients, thereby facilitating more efficient training and improving the overall convergence of the model. Adding skip connections proved helpful in training the model, as it improved the performance and reached convergence faster. A further introduction of a learnable parameter did not achieve any meaningful result.\n",
    "\n",
    "After training a clear issue emerged, as the addition of the topological features alters significantly the structure of the network, even when the parameters are initialized at very low values. Therefore, the network outputs do not replicate the ones from base CLIP when the Topological prompt parameters are initialized at 0. This led the network to overfit on the training data while losing the ability to classify the novel classes. Additionally, the network varies greatly in performance between different classes, suggesting that the prompt extraction methodology works differently for different kinds of input.\n",
    "\n",
    "Since TopoVPT does not produce good results on the novel classes, thresholding was employed, in order to benefit from both the zero-shot performance of CLIP and the improvements achieved by the model.\n",
    "\n",
    "#### Test set performance for selected models\n",
    "\n",
    "We propose two versions of TopoVPT, one where only the added parameters are trained and the other where the parameters of the third and fourth layer of the Modified ResNet are also trained. Based on validation results, it has been noticed that the best performance is obtained by training for 20 epochs using AdamW with an initial learning rate of 0.005 and a weight decay of 0.01. A decrease in the weight decay does not yield better results. The threshold hyperparameters have been selected based on validation set performance.\n",
    "\n",
    "\n",
    "#### First proposal, 20 Iterations, 0.013 Threshold\n",
    "| **Metric**       | **Value (%)** |\n",
    "|------------------|---------------|\n",
    "| Base Accuracy    | 63.4          |\n",
    "| Novel Accuracy   | 68.2          |\n",
    "| Harmonic Mean    | 65.7          |\n",
    "\n",
    "\n",
    "#### Second proposal, 20 Iterations, 0.0165 Threshold\n",
    "| **Metric**       | **Value (%)** |\n",
    "|------------------|---------------|\n",
    "| Base Accuracy    | 72.7          |\n",
    "| Novel Accuracy   | 66            |\n",
    "| Harmonic Mean    | 69.2          |\n",
    "\n",
    "TopoVPT with thresholding shows results which are slightly worse than CLIP. This is likely due to the fact that more data is required to properly train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOBeHM21a22d"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "The only model that produced satisfying results is the one based on confidence thresholding. It is very fast to train and produces a clear improvement over base CLIP, whithout needlessly increasing the performance overhead at test time. With a more thorough parallel implementation of the forward pass the model should be a direct upgrade over base CLIP in few-shot adaptation scenarios. This improvement is further reinforced by the ease of putting the emphasis on either base or novel classes by modifying the threshold parameter.\n",
    "\n",
    "The other methodologies we employed all had a number of issues. Gaussian clustering didn't show much promise when trained on very little amound of data. The hierarchical transformation-discriminating generative model proved too hard to train in terms of time and hardware resources. Finally, the approach based on Topological Visual Prompt Tuning was not able to maintain novel-class performance during training like we had hoped. And even when combined with thresholding, the method proved not to be adapt to an application with only a small number of training examples.\n",
    "\n",
    "In conclusion confidence-based thresholding proved to be effective, despite its simplicity, while other, more complex, methods were not suitable for the task at hand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWSZkmn-a22d"
   },
   "source": [
    "## Bibliography\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RjwcfRNa22e"
   },
   "source": [
    "1. Henry Adams, Sofya Chepushtanova et al. 2015. 'Persistence Images: A Stable Vector Representation of Persistent Homology'. [link](https://arxiv.org/abs/1507.06217)\n",
    "2. Rubén Ballester, Carles Casacuberta, Sergio Escalera. 2023. 'Topological Data Analysis for Neural Network Analysis: A Comprehensive Survey'. [link](https://arxiv.org/abs/2312.05840)\n",
    "3. Menglin Jia, Lumin Tang et al. 2022. 'Visual Prompt Tuning'. [link](https://arxiv.org/abs/2203.12119)\n",
    "4. Shelly Sheynin, Sagie Benaim, Lior Wolf. 2021. 'A Hierarchical Transformation-Discriminating Generative Model for Few Shot Anomaly Detection'. [link](https://arxiv.org/abs/2104.14535)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
